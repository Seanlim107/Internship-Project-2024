{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c25d784",
   "metadata": {},
   "source": [
    "# Inference Testing for Baseline Model + 3D Guesser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4746ebe",
   "metadata": {},
   "source": [
    "## Date: 27th August 2024\n",
    "## Creator: Sean Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6267ec",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb514a",
   "metadata": {},
   "source": [
    "The following notebook displays the baseline model used for distance estimation\n",
    "\n",
    "The code consists of the following pipeline:\n",
    "1) Yolov10 model: Used for detecting objects and is the core of the model\n",
    "2) Config files: Extracts important manually input information such as real dimsnesions and the intrinsic camera matrix\n",
    "- Real dimensions: Estimates in meters (m) used to compare the bounding box vs the real length, height of the object\n",
    "- Intrinsic camera matrix: setting \"use_own\" = True allows manually input variables, while False uses the image dimensions as the camera matrix variables (not recommended)\n",
    "3) The closest distance between two bounding boxes is obtained in pixel length and transforms them to 3D coordinates using computer vision transformations, obtaining the distance between the object and the camera and used as the depth to then get the location of the object (X, Y, Z=depth)\n",
    "4) Euclidean distance between the center points of the boxes are obtained and if it is less then the configurable [safe_distance], a warning will be sent out and lines will be drawn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f876f",
   "metadata": {},
   "source": [
    "### Install Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66532a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained YOLOv10n model\n",
    "from ultralytics import YOLOv10\n",
    "from lib.Dataset import ConstructionDataset\n",
    "from lib.Camera import Camera\n",
    "from lib.utils import parse_yaml, estimate_distance_2d, draw_lines, estimate_distance_centers_3d\n",
    "from lu_vp_detect import VPDetection\n",
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "# !python --version\n",
    "from lu_vp_detect import VPDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=parse_yaml('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd076e8e",
   "metadata": {},
   "source": [
    "### Prepare dataset from original repo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe458e5a",
   "metadata": {},
   "source": [
    "#### skips if config > YoloPrep > init = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python YoloCreate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494df684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialization\n",
    "weightpath = os.path.join('runs/detect/train12/weights/best.pt')\n",
    "model = YOLOv10(weightpath)\n",
    "lookup = config['Classes']\n",
    "config_cam = config['Camera']\n",
    "\n",
    "batch_size = 1 # Only for testing purposes\n",
    "safe_distancing = config['General']['safe_dist']\n",
    "consDataset = ConstructionDataset(config, crops=False)\n",
    "\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "            'shuffle':True,\n",
    "            'num_workers': 6}\n",
    "seed = config['General']['seed']\n",
    "if seed is not None:\n",
    "    torch.manual_seed(config['General']['seed'])\n",
    "generator = data.DataLoader(consDataset, **params)\n",
    "\n",
    "# Drawing parameters\n",
    "Color_palette = 255 * np.eye(3)\n",
    "config_cam = config['Camera']\n",
    "config_vp = config['VP_Detector']\n",
    "remove_fisheye=config_cam['remove_fisheye']\n",
    "# BGR format\n",
    "# First row is red, second green, third blue\n",
    "Color_palette = Color_palette[:, ::-1].astype(int).tolist()\n",
    "\n",
    "length_thresh = config_vp['length_thresh']\n",
    "principal_point = config_vp['principal_point']\n",
    "focal_length = config_vp['focal_length']\n",
    "seed = config['General']['seed'] # Or specify whatever ID you want (integer)\n",
    "\n",
    "vpd = VPDetection(length_thresh, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3459a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise generator\n",
    "generator_iter = iter(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15507668",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b0565",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(local_image_tensor, local_image, indexed_label, indexed_pair) = next(generator_iter)\n",
    "\n",
    "temp_img = local_image_tensor\n",
    "temp_img_ori = np.array(local_image[0])\n",
    "\n",
    "print(temp_img_ori.shape)\n",
    "\n",
    "cam = Camera(use_own = config_cam['use_own'], img=temp_img_ori, distortion_coef=config_cam['distortion_coef'], fx=config_cam['fx'], fy=config_cam['fy'], cx=config_cam['cx'], cy=config_cam['cy'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(temp_img)\n",
    "\n",
    "#Predicted coordinates of box (top left, bottom right)\n",
    "list_boxes = results[0].boxes.xyxy\n",
    "\n",
    "#List of detected classes\n",
    "detected_classes = results[0].boxes.cls\n",
    "\n",
    "# Plot original Image with Yolo Detection    \n",
    "img = results[0].plot()\n",
    "\n",
    "#Draw lines and send warning if a distance is lower than safe distance\n",
    "if(len(list_boxes)>1 and 0 in detected_classes):\n",
    "    list_conf = results[0].boxes.conf\n",
    "    list_workers = [(i, detected_classes[i].item(), list_boxes[i], list_conf[i].item()) for i in range(len(detected_classes)) if int(detected_classes[i]) == 0]\n",
    "    list_nonworkers = [(i, detected_classes[i].item(), list_boxes[i], list_conf[i].item()) for i in range(len(detected_classes)) if int(detected_classes[i]) != 0]\n",
    "\n",
    "##################################################################################################################################################\n",
    "# List workers and List Non Workers have the following:\n",
    "# (0: index wrt detected objects in YOLO accordingly, 1: Class in int format, 2: xyxy, 3: Confidence Score for ease of labelling when plotting)\n",
    "##################################################################################################################################################\n",
    "\n",
    "    vps = vpd.find_vps(temp_img_ori)\n",
    "    # print(vps)\n",
    "    # print(vpd.vps_2D)\n",
    "\n",
    "    vp1, vp2, vp3 = vpd.vps_2D[:3]\n",
    "\n",
    "    # draw_grid(img, vp1, vp2, vp3, Color_palette)\n",
    "\n",
    "    for worker in list_workers:\n",
    "        for nonworker in list_nonworkers:\n",
    "            length= estimate_distance_2d(worker[2], nonworker[2])\n",
    "            # print(length)\n",
    "            hazard = lookup[nonworker[1]]['name']\n",
    "            worker_dim = lookup[worker[1]]['dimensions']\n",
    "            hazard_dim = lookup[nonworker[1]]['dimensions']\n",
    "            \n",
    "            worker_3d_coords = cam.find_real_coords(worker[2], worker_dim)\n",
    "            hazard_3d_coords = cam.find_real_coords(nonworker[2], hazard_dim)\n",
    "            distance = estimate_distance_centers_3d(worker_3d_coords, hazard_3d_coords)\n",
    "            \n",
    "            \n",
    "            # print(f'3D Coordinates of worker: ,{worker[3]:.2f},{worker_3d_coords}')\n",
    "            # print(f'3D Coordinates of , {hazard}: , {nonworker[3]:.2f}, {hazard_3d_coords}')\n",
    "            # print(f'Distance = {distance:.2f}m')\n",
    "            \n",
    "            if(distance < safe_distancing):\n",
    "                print(f'Unsafe distancing between worker:{worker[3]:.2f} and {hazard}:{nonworker[3]:.2f}, Distance={distance:.2f}m')\n",
    "                print('3D Coordinates of worker: ',worker[3],worker_3d_coords)\n",
    "                print('3D Coordinates of ', hazard,': ', nonworker[3], hazard_3d_coords)\n",
    "            \n",
    "            \n",
    "            draw_lines(worker[2], nonworker[2], img, distance, debug)\n",
    "            \n",
    "    print('__________________________________________________________________________')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ffc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23a8dcaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252a204e",
   "metadata": {},
   "source": [
    "# 3D Guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62499a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lib.Dataset import ConstructionDataset\n",
    "# from lib.Camera import Camera\n",
    "from lib.utils import parse_yaml, get_angle, save_checkpoint, load_checkpoint, draw_lines, estimate_distance_centers_3d\n",
    "# from lib.loss import Custom_Loss\n",
    "from lib.VP_Detector import VPD\n",
    "from torchvision.models import vgg, efficientnet_b0\n",
    "from my_model.Model import BB_Guesser_Model\n",
    "from lib.loss import Custom_Loss_v2\n",
    "# from lib.traintestfuncs import train\n",
    "# from model.faster_rcnn.resnet import resnet\n",
    "from ultralytics import YOLOv10\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "import numpy as np\n",
    "from lib_3D.Dataset import *\n",
    "from lib_3D.Plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd783055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fdc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=parse_yaml('config.yaml')\n",
    "lookup = config['Classes']\n",
    "batch_size = 1 # Only for testing purposes\n",
    "seed = config['General']['seed']\n",
    "safe_distancing = config['General']['safe_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c947f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consDataset = ConstructionDataset(config, crops=False)\n",
    "    \n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "            'shuffle':True,\n",
    "            'num_workers': 6}\n",
    "if(seed is not None):\n",
    "    torch.manual_seed(config['General']['seed'])\n",
    "generator = data.DataLoader(consDataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c3c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sean\\anaconda3\\envs\\Construction\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "c:\\Users\\Sean\\Documents\\GitHub\\Interns_2024\\lib\\utils.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file_name, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's pretrained weights loaded!\n"
     ]
    }
   ],
   "source": [
    "weightpath = os.path.join('runs/detect/train12/weights/best.pt')\n",
    "yolo = YOLOv10(weightpath)\n",
    "\n",
    "# backbone = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "backbone = vgg.vgg19_bn(weights='IMAGENET1K_V1')\n",
    "model = BB_Guesser_Model(backbone=backbone, proposals=1, angles=1)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# loss_fn = Custom_Loss_v2()\n",
    "file_name = \"3D_Guesser_Train_ckpt_6.pth\"\n",
    "# file_name = \"3D_Guesser_Train_ckpt_4.pth\"\n",
    "# file_name = '3D_Guesser_Train_ckpt_24.pth'\n",
    "_, _ = load_checkpoint(model, file_name, optimizer)\n",
    "my_vpd = VPD(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322096e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8da2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_iter = iter(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c26470",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Construction (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n Construction ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "(indexed_img_tensor, indexed_ori_img, indexed_label, indexed_pair) = next(generator_iter)\n",
    "curr_image = indexed_img_tensor\n",
    "plot_image = np.array(indexed_ori_img.squeeze())\n",
    "# curr_image = indexed_img_tensor.to(device)\n",
    "# indexed_orientation = indexed_orientation.to(device)\n",
    "# curr_crop = indexed_crop_tensor.to(device)\n",
    "# indexed_dims = indexed_dims.to(device)\n",
    "list_workers2 = []\n",
    "list_nonworkers2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    detections = yolo(curr_image)\n",
    "# img = np.copy(plot_image)\n",
    "# print(detections)\n",
    "calib_file = \"calib_cam_to_cam.txt\"\n",
    "\n",
    "list_boxes_2 = detections[0].boxes.xyxy\n",
    "\n",
    "#List of detected classes\n",
    "detected_classes_2 = detections[0].boxes.cls\n",
    "\n",
    "img = detections[0].plot()\n",
    "\n",
    "for detection in detections[0]:\n",
    "    # print(img, detection.boxes.cls, detection.boxes.xyxy, calib_file)\n",
    "    # print(img.shape, detection.boxes.cls.shape, detection.boxes.xyxy.shape)\n",
    "    box = detection.boxes.xyxy.squeeze().cpu().int().numpy()\n",
    "    box_2d = ([(box[0], box[1]), (box[2], box[3])])\n",
    "    try:\n",
    "        detectedObject = DetectedObject(img, detection.boxes.cls.cpu(), box_2d, calib_file)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    theta_ray = detectedObject.theta_ray\n",
    "    input_img = detectedObject.img\n",
    "    proj_matrix = detectedObject.proj_matrix\n",
    "    box_2d = box_2d\n",
    "    \n",
    "    \n",
    "    # detected_class = detection.boxes.cls\n",
    "    \n",
    "    input_tensor = torch.zeros([1,3,224,224]).to(device)\n",
    "    input_tensor[0,:,:,:] = input_img\n",
    "    \n",
    "    [orient, dim] = model(input_tensor)\n",
    "    # print(orient, dim)\n",
    "    orient = orient.cpu().data.numpy()[0, :, :]\n",
    "    \n",
    "    # conf = conf.cpu().data.numpy()[0, :]\n",
    "    dim = dim.cpu().data.numpy()[0, :]\n",
    "    \n",
    "    orient = orient[0,:]\n",
    "    # print(orient)\n",
    "    # argmax = np.argmax(conf)\n",
    "    # orient = orient[argmax, :]\n",
    "    cos = np.cos(orient[0])\n",
    "    sin = np.sin(orient[0])\n",
    "    alpha = np.arctan2(sin, cos)\n",
    "    # print(alpha)\n",
    "    # alpha = orient[0]\n",
    "    beta = 0\n",
    "    # beta = orient[1]\n",
    "    beta = 0\n",
    "    # alpha += angle_bins[argmax]\n",
    "    # alpha -= np.pi\n",
    "    # print(alpha)\n",
    "    \n",
    "    # print(box_2d)\n",
    "    detect_class = detection.boxes.cls.cpu().int().numpy()[0]\n",
    "    dim = np.array(lookup[detect_class]['dimensions'])\n",
    "    dim = np.flip(dim)\n",
    "    # print(dim)\n",
    "    location = plot_regressed_3d_bbox(img, proj_matrix, box_2d, dim, alpha, theta_ray, beta=beta, clip=True )\n",
    "    # print(location)\n",
    "    \n",
    "    detect_label = lookup[detect_class]['name']\n",
    "    detect_conf = detection.boxes.conf.cpu().float().numpy()[0]\n",
    "    # print(detect_class)\n",
    "    if(detect_class == 0):\n",
    "        list_workers2.append((detect_conf, location, detect_class, box, dim))\n",
    "    else:\n",
    "        list_nonworkers2.append((detect_conf, location, detect_class, box, dim))\n",
    "    print(f'Estimated pose:{detect_label} {detect_conf:.2f} {location}')\n",
    "# numpy_vertical = np.concatenate((plot_image, img), axis=0)\n",
    "\n",
    "\n",
    "if(len(list_nonworkers2) > 0 and len(list_workers2) > 0):\n",
    "    for worker in list_workers2:\n",
    "        for nonworker in list_nonworkers2:\n",
    "            conf_worker, coord_worker, class_worker, box_worker, dim = worker\n",
    "            conf_non, coord_non, class_non, box_non, dim = nonworker\n",
    "            # length= estimate_distance_2d(worker[2], nonworker[2])\n",
    "            # print(length)\n",
    "            hazard = lookup[class_non]['name']\n",
    "            \n",
    "            worker_3d_coords = coord_worker\n",
    "            hazard_3d_coords = coord_non\n",
    "            distance = estimate_distance_centers_3d(worker_3d_coords, hazard_3d_coords, dim)\n",
    "            \n",
    "            \n",
    "            # print(f'3D Coordinates of worker: ,{worker[3]:.2f},{worker_3d_coords}')\n",
    "            # print(f'3D Coordinates of , {hazard}: , {nonworker[3]:.2f}, {hazard_3d_coords}')\n",
    "            # print(f'Distance = {distance:.2f}m')\n",
    "            \n",
    "            if(distance < safe_distancing):\n",
    "                print(f'Unsafe distancing between worker:{conf_worker:.2f} and {hazard}:{conf_non:.2f}, Distance={distance:.2f}m')\n",
    "                # print('3D Coordinates of worker: ',conf_worker,worker_3d_coords)\n",
    "                # print('3D Coordinates of ', hazard,': ', conf_non, hazard_3d_coords)\n",
    "                color = (255,0,0)\n",
    "            else:\n",
    "                color = (255,255,255)\n",
    "            \n",
    "            \n",
    "            \n",
    "            draw_lines(box_worker, box_non, img, distance, debug=debug, safe_distancing=safe_distancing)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c59b18c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pump truck 0.8050343 [-1.4354035884831169, 4.276195028252108, 18.947894239312184] [490 294 621 407]\n",
      "worker 0.50111413 [-0.2747990848972066, 7.720346271042832, 24.32680479259329] [589 393 613 433]\n",
      "42.14185933715964\n"
     ]
    }
   ],
   "source": [
    "conf_worker, coord_worker, class_worker, box_worker, dim_worker = list_workers2[2]\n",
    "conf_non, coord_non, class_non, box_non, dim_non = list_nonworkers2[3]\n",
    "\n",
    "print(lookup[class_non]['name'], conf_non, coord_non, box_non)\n",
    "print('worker', conf_worker, coord_worker, box_worker)\n",
    "# length= estimate_distance_2d(worker[2], nonworker[2])\n",
    "# print(length)\n",
    "hazard = lookup[class_non]['name']\n",
    "\n",
    "worker_3d_coords = coord_worker\n",
    "hazard_3d_coords = coord_non\n",
    "distance = estimate_distance_centers_3d(worker_3d_coords, hazard_3d_coords, dim1=dim_worker, dim2=dim_non)\n",
    "\n",
    "# print(lookup[class_non])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbd672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973c4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Construction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
